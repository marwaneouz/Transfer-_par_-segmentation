# Transfer-_par_-segmentation
# üß† Apprentissage par Transfert pour la Segmentation d'Images

![Python](https://img.shields.io/badge/Python-3.7%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![PyTorch](https://img.shields.io/badge/PyTorch-1.8%2B-red)
![License](https://img.shields.io/badge/License-MIT-green)

Ce d√©p√¥t pr√©sente un travail pratique (TP) sur **l'apprentissage par transfert appliqu√© √† la segmentation s√©mantique d'images**, en utilisant des architectures modernes comme **U-Net**, **DeepLabV3+**, et la g√©n√©ration de donn√©es avec des **GANs**. Le tout est impl√©ment√© en Python avec TensorFlow/Keras et PyTorch.

‚û°Ô∏è **Objectif** : Explorer, comparer et am√©liorer des mod√®les de segmentation sur le dataset **COCO**, en appliquant des techniques avanc√©es comme l‚Äôattention, le transfer learning, et l‚Äôaugmentation de donn√©es g√©n√©ratives.

üîó **Acc√©der au rapport complet** : [Rapport PDF](transfer_segmentation.pdf)  
üíª **Code source** : [https://github.com/marwaneouz/Transfer-_par_-segmentation](https://github.com/marwaneouz/Transfer-_par_-segmentation)

---

## üìö Sommaire

- [Objectifs](#objectifs)
- [Dataset utilis√©](#dataset-utilis√©-coco)
- [Mod√®les impl√©ment√©s](#mod√®les-impl√©ment√©s)
- [M√©triques d'√©valuation](#m√©triques-d√©valuation)
- [R√©sultats principaux](#r√©sultats-principaux)
- [Structure du d√©p√¥t](#structure-du-d√©p√¥t)
- [Comment ex√©cuter](#comment-ex√©cuter)
- [Optimisations](#optimisations)
- [Conclusion](#conclusion)
- [R√©f√©rences](#r√©f√©rences)

---

## üéØ Objectifs

- Impl√©menter et comparer des mod√®les de segmentation : **U-Net**, **U-Net avec attention**, **DeepLabV3+**.
- Appliquer le **transfert learning** en utilisant des backbones pr√©-entra√Æn√©s (ResNet50, ResNet101, Xception).
- Am√©liorer les performances avec des techniques comme l‚Äô**attention**, les **connexions saut√©es**, et les **pertes combin√©es**.
- G√©n√©rer de nouvelles images d'entra√Ænement avec des **GANs** (notamment SAGAN).
- Optimiser l'entra√Ænement avec **mixed precision**, **augmentation avanc√©e**, et **mod√®les d'ensemble**.

---

## üñºÔ∏è Dataset utilis√© : COCO

Le dataset [**COCO (Common Objects in Context)**](https://cocodataset.org) est utilis√© pour l'entra√Ænement et l'√©valuation :

- 80 classes d'objets courants
- Images haute r√©solution avec annotations de segmentation pr√©cises
- Id√©al pour tester des mod√®les de segmentation s√©mantique et d'instances

üîó Officiel : [https://cocodataset.org](https://cocodataset.org)

---

## üèóÔ∏è Mod√®les impl√©ment√©s

| Mod√®le | Description |
|-------|-------------|
| **U-Net (basique)** | Architecture encodeur-d√©codeur avec skip connections |
| **U-Net + Attention** | Ajout de m√©canismes d'attention pour mieux localiser |
| **U-Net + ResNet50** | Encoder bas√© sur ResNet50 pr√©-entra√Æn√© sur ImageNet |
| **DeepLabV3+ (Xception)** | Utilise ASPP et d√©codeur pour segmentation fine |
| **DeepLabV3+ (ResNet101)** | Version am√©lior√©e avec meilleur contexte |
| **GAN (Pix2Pix/SAGAN)** | G√©n√©ration d'images pour augmentation de donn√©es |

---

## üìä M√©triques d'√©valuation

Les mod√®les sont √©valu√©s avec les m√©triques suivantes :

- **Dice Similarity (DS)** : Mesure de superposition entre masques
- **IoU (Intersection over Union)** : Qualit√© de la segmentation
- **Pr√©cision & Rappel** : Performance par classe
- **Accuracy** : Taux de pixels correctement classifi√©s
- **mIoU** : Moyenne de l'IoU sur toutes les classes

---

## üìà R√©sultats principaux

| Mod√®le | Pr√©cision | mIoU | Temps/√©poque |
|--------|---------|-------|-------------|
| U-Net (basique) | 78.5% | 0.654 | 25 min |
| U-Net + Attention | 81.7% | 0.702 | 28 min |
| U-Net + ResNet50 | 84.1% | 0.728 | 32 min |
| DeepLabV3+ (Xception) | 86.3% | 0.751 | 45 min |
| **DeepLabV3+ (ResNet101)** | **87.1%** | **0.763** | 52 min |

‚úÖ **Meilleur mod√®le** : **DeepLabV3+ avec ResNet101**

---

 
